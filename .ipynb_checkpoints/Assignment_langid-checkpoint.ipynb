{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uOlDE4gfCw-R"
   },
   "source": [
    "# Assignment: Language identification with trigrams\n",
    "\n",
    "Languages differ in their syllable structure, spelling conventions and common vocabulary. \n",
    "As a result, letter sequences from each language have distinct, characteristic frequencies \n",
    "that allow us to identify the language of a text on the basis of its ngram profile.\n",
    "\n",
    "In this assignment we build a trigram language recognizer. The recognizer works by comparing the ngram profile of a test document against a collection of ngram profiles for known languages, computed and saved in advance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "TableOfContents": 1,
    "colab_type": "text",
    "id": "6OF8IDWRCw-U"
   },
   "source": [
    "## Contents\n",
    "\n",
    "\n",
    "**[1. Overview](#1.-Overview)**  \n",
    "\n",
    "**[2. General requirements](#2.-General-requirements)**  \n",
    "\n",
    "**[3. Preparation: The data files](#3.-Preparation:-The-data-files)**  \n",
    "\n",
    "**[4. The `langdetect` module](#4.-The-langdetect-module)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [4.1 `prepare()`](#4.1-prepare%28%29)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [4.2 `trigrams()`](#4.2-trigrams%28%29)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [4.3 `trigram_table()`](#4.3-trigram_table%28%29)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [4.4 `read_trigrams()` and `write_trigrams()`](#4.4-read_trigrams%28%29-and-write_trigrams%28%29)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [4.5 `cosine_similarity()`](#4.5-cosine_similarity%28%29)  \n",
    "\n",
    "**[5. Script 1: Create profiles](#5.-Script-1:-Create-profiles)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [5.1 The data](#5.1-The-data)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [5.2 The function `make_profiles()`](#5.2-The-function-make_profiles%28%29)  \n",
    "\n",
    "**[6. Script 2: The language recognizer](#6.-Script-2:-The-language-recognizer)**  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [6.1 The `LangMatcher` class](#6.1-The-LangMatcher-class)  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "  [6.2 The recognizer main script](#6.2-The-recognizer-main-script)  \n",
    "\n",
    "**[7. Script 3: Evaluation](#7.-Script-3:-Evaluation)**  \n",
    "\n",
    "**[8. Submission](#8.-Submission)**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xi_E-cutCw-W"
   },
   "source": [
    "## 1. Overview\n",
    "\n",
    "Your trigram recognizer will consist of four files. Step by step specifications are given below. **Study the specifications carefully and make sure your work meets all the requirements.**\n",
    "\n",
    "This overview summarizes the sections that follow. If there seems to be disagreement between this summary and the detailed description, follow the detailed description.\n",
    "\n",
    "\n",
    "### Core functionality: The module `langdetect.py`\n",
    "\n",
    "This module will define some important core functions, and any other helper functions you find useful. They include:\n",
    "\n",
    "- `trigram_table()`: Calculate and _return_ a trigram frequency table from a string.\n",
    "- `read_table()` and `write_table()`: Functions that read, resp. write, a list of trigrams and their frequencies.\n",
    "- `cosine_similarity()`: Return the cosine similarity score between two frequency tables.\n",
    "\n",
    "\n",
    "### Three scripts: Create profiles, match languages\n",
    "\n",
    "All three scripts should import necessary functions from the module `langdetect`. Do not copy-paste code or reimplement any operations that `langdetect.py` can already handle.\n",
    "\n",
    "1. `write_profiles.py`: reads a directory of multilingual files and saves trigram frequency tables for later use. \n",
    "\n",
    "2. `matchlang.py`: The language recognizer. This is a general-purpose program with command-line arguments and options. It can also be imported as a module, allowing the recognizer to be used by another program.\n",
    "\n",
    "3. `evaluate.py`: This script will apply the recognizer to two collections of test files, and measure and report the success of the recognition process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aGXZWcXMCw-Z"
   },
   "source": [
    "## 2. General requirements\n",
    "\n",
    "Read the instructions carefully. Many of the necessary concepts and \n",
    "skills are covered in the forthcoming reading and practica (up to the \n",
    "deadline).\n",
    "\n",
    "**Credit.** Projects will be graded on function (correct results, design according\n",
    "to the specification) as well as form (organization and appropriateness \n",
    "of the code, documentation). **Test** the parts of your program to\n",
    "ensure that they work as intended. \n",
    "\n",
    "**Specification.** Ensure that your modules and functions **exactly** meet the specifications given here, including naming and arguments. (Extra _optional_ arguments are fine.) Adherence to specifications is essential for the components of a complex system to work together. You are free (and encouraged) to structure your code by defining additional functions, optional arguments, etc. If you wish to go beyond the requirements of the assignment, you are welcome to. Just ensure that your additions do not conflict with the specifications. \n",
    "\n",
    "**Module structure.** When imported, your modules should only provide definitions; they must not read or write data files, generate output, or carry out any lengthy calculations. Modules intended to be used as stand-alone scripts must have a conditional section that executes the code. (For modules that don't have such use, you are free to add a conditional section to help with development and debugging.) \n",
    "\n",
    "**Paths.** Do not use absolute paths for files. Use arguments rather than hard-coded names wheverer possible. Use forward slashes (\"/\") to separate components. Unlike backslashes (\"\\\"), they do not need escaping and they work on both Windows and Unix (OS X, Linux) systems.\n",
    "\n",
    "**Form.** Follow our recommended best practices on variable naming, line length, and \"docstrings\" for all functions and modules. Functions should avoid relying on global variables; use function arguments to pass information. Avoid code repetition; if you find it useful to copy and paste your code to more than one place, you should factor it out into a function. Finally, if you are aware that some part of your code has a problem but are unable to fix it, document the problem in a comment.\n",
    "\n",
    "**Function.** In addition to producing correct results, beware of excessively long running times. They indicate a mistake in your algorithm, or an unsuitable data structure somewhere.\n",
    "\n",
    "\n",
    "Finally, be aware that **your code will be evaluated with a different data set.** Do not hard-code paths and filenames in your code, except as directed. Ensure that your code pays attention to function arguments and command-line arguments. Make sure that the code runs correctly when a non-default value for any argument is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JnZk_sxrCw-c"
   },
   "source": [
    "## 3. Preparation: The data files\n",
    "\n",
    "Download the file `datafiles.zip`, unzip it and move the subdirectories `training` and `test-clean` to the folder that will contain your scripts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yIf4IX1ZCw-c"
   },
   "source": [
    "### Automated tests\n",
    "\n",
    "This notebook contains snippets of test code. If you also place it in the same folder as your code and data folders, you can check your work by running the test cells. Obviously, **you must write the code before you can run the tests.** \n",
    "\n",
    "**Important:** Python will only import your modules once. If you modify your files, you must restart the notebook kernel for the changes to take effect. You can use the keyboard shortcut `<Escape> 0 0` to restart the kernel (possibly followed by `Control-Return` to rerun the current cell), or copy the tests into their own script that you can run separately.\n",
    "\n",
    "The tests are only partial: **Passing all the tests doesn't guarantee that your code is correct in all respects.** Also, the tests do not check things like documentation and code style. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ViTDHwzlCw-f"
   },
   "source": [
    "## 4. The `langdetect` module\n",
    "\n",
    "Create a Python module `langdetect.py` that defines the following functions:\n",
    "\n",
    "### 4.1 `prepare()`\n",
    "\n",
    "The function `prepare(text)` takes a string, replaces the characters `!?\",.()<>` with spaces, and then splits the new string on whitespace and returns the resulting list of words.\n",
    "Run the following code to check if your function seems to work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kGDi-HsHCw-h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'cough', 'cough', 'HAL-9000', \"Don't\", 'touch']\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "# Demo:\n",
    "from langdetect import prepare\n",
    "tokens = prepare('This is <cough,cough> \"HAL-9000\".  Don\\'t touch!')\n",
    "print(tokens)\n",
    "\n",
    "# Test:\n",
    "if tokens == ['This', 'is', 'cough', 'cough', 'HAL-9000', \"Don't\", 'touch']:\n",
    "    print(\"ok\")\n",
    "else:\n",
    "    import sys\n",
    "    print(\"Incorrect result\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ykWP29PoCw-n"
   },
   "source": [
    "### 4.2 `trigrams()`\n",
    "\n",
    "The function `trigrams(seq)` takes any sequence (e.g., a string) and returns a list of its trigrams. It must not apply any padding, splitting or other modifications to its argument. Example and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SztfTmDeCw-p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['R2.', '2.D', '.D2']\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "from langdetect import trigrams\n",
    "tr = trigrams(\"R2.D2\")\n",
    "print(tr)\n",
    "        \n",
    "# Test:\n",
    "if tr == ['R2.', '2.D', '.D2']:\n",
    "    print(\"ok\")\n",
    "else:\n",
    "    import sys\n",
    "    print(\"Incorrect result\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UPCUOuSBCw-u"
   },
   "source": [
    "### 4.3 `trigram_table()`\n",
    "\n",
    "We will represent a table of trigram frequencies as a dictionary whose keys are ngrams and the values are frequencies (ngram counts). The function `trigram_table(text, limit)` takes two arguments: A string to process and the _optional_ argument `limit`, giving the number of frequent ngrams to return. The default value must be 0, which means all ngrams should be returned. This function should:\n",
    "        \n",
    "1. Use `prepare()` to clean and tokenize the text.\n",
    "- Surround each tokenized word with the characters `<` and `>` (angle brackets). E.g., `\"it's\"` becomes `\"<it's>\"`.\n",
    "    \n",
    "- Extract the trigrams of each word, and use a dictionary (either an ordinary `dict` or `collections.Counter`) to count how often each trigram occurs.\n",
    "\n",
    "- If `limit` is 0, return the entire dictionary.\n",
    "\n",
    "- Otherwise `limit` must be an integer that tells us how many ngrams to return (starting from the most frequent). Create and return a new dictionary containing this amount of keys and values. \n",
    "\n",
    "#### **Notes**\n",
    "\n",
    "* If the cutoff point falls among several ngrams with the same frequency, the selection is arbitrary (and might vary between program runs).\n",
    "\n",
    "2. One way to select a subset of the entries in a dictionary is to convert it to a list of ordered pairs, sorted by frequency, and create a new dictionary from a slice of the values. You are free to use any approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qzV2ir8vCw-w"
   },
   "source": [
    "**Example and test:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vQq3LgcfCw-w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<ah': 4, 'ah>': 4, 'ep>': 3, '<hi': 2, 'hie': 2, 'iep': 2, 'en>': 2, '<ho': 1, 'hoe': 1, 'oer': 1, 'era': 1, 'ra>': 1, '<Se': 1, 'Sep': 1, '<is': 1, 'is>': 1, '<ee': 1, 'een': 1, '<mo': 1, 'moo': 1, 'ooi': 1, 'oie': 1, 'ie>': 1, '<jo': 1, 'jon': 1, 'ong': 1, 'nge': 1, 'gen': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Incorrect result\n"
     ]
    }
   ],
   "source": [
    "from langdetect import trigram_table\n",
    "from langdetect import write_trigrams\n",
    "import os.path\n",
    "\n",
    "top = trigram_table(\"hiep, hiep, hoera! Sep is een mooie jongen ah ah ah ah.\")\n",
    "print(top)\n",
    "\n",
    "if top == {'<hi': 2, 'hie': 2, 'ep>': 2, 'iep': 2}:\n",
    "    print(\"ok\")\n",
    "else:\n",
    "    import sys\n",
    "    print(\"Incorrect result\", file=sys.stderr)\n",
    "    \n",
    "write_trigrams(top, \"tekst.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mx-dK_KOCw-0"
   },
   "source": [
    "### 4.4 `read_trigrams()` and `write_trigrams()`\n",
    "\n",
    "The function `write_trigrams(table, filename)` takes an ngram table as generated by `trigram_table()` and a filename, and writes the ngrams to that file, in this format:\n",
    "\n",
    "    2 iep \n",
    "    2 ep>\n",
    "    1 <Hi\n",
    "    1 hoe\n",
    "    1 <ho\n",
    "    ...\n",
    "    \n",
    "The output must use the `utf8` encoding, and must be in descending order of frequency.\n",
    "\n",
    "The function `read_trigrams(filename)` must reverse the process: It opens the file for reading with `utf8` encoding, it reads the file's contents and converts them to dictionary whose values are **integers**, not strings.  \n",
    "\n",
    "Check that you can \"round-trip\" your data with these functions, i.e. read back exactly what you wrote out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Louu8BgICw-2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ove': 6, 'ver': 6, 'rov': 5, 'val': 4, '<éé': 3, 'één': 3, 'én>': 3, '<ro': 3, 'er>': 3, '<va': 2}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'write'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-733268dff32a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrigram_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mwrite_trigrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rover.10.TEMP\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mreread_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_trigrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"rover.10.TEMP\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\ComputatieTrekkers\\langdetect.py\u001b[0m in \u001b[0;36mwrite_trigrams\u001b[1;34m(table, filename)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mwrite_trigrams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mtablefile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[0mtablefile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mtablefile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'write'"
     ]
    }
   ],
   "source": [
    "from langdetect import trigram_table, read_trigrams, write_trigrams\n",
    "\n",
    "text = \"Het valt voor, dat bij één roveroverval, één rover voorover over één roverval valt.\"\n",
    "table = trigram_table(text, 10)\n",
    "print(table)\n",
    "write_trigrams(table, \"rover.10.TEMP\")\n",
    "reread_table = read_trigrams(\"rover.10.TEMP\")\n",
    "\n",
    "if table == reread_table:\n",
    "    print(\"ok\")\n",
    "else:\n",
    "    import sys\n",
    "    print(\"The round trip fails\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iPoeQo9wCw-8"
   },
   "source": [
    "### 4.5 `cosine_similarity()`\n",
    "\n",
    "The function `cosine_similarity(known, unknown)` takes two arguments, which must both be dictionaries of ngram frequencies: the ngrams for the unknown text we want to identify, and one of the reference profiles of a known language. It must compute and return the \"cosine similarity\" metric between the two tables.\n",
    "\n",
    "The formula of cosine similarity between *a* and *b* is as follows:\n",
    "\n",
    "$\\cos(a,b)=\\frac{a\\cdot b}{magnitude(a)*magnitude(b)}$\n",
    "\n",
    "where $a\\cdot b$ is the _dot product_ of $a$ and $b$ which equals $\\Sigma_i a_i*b_i$ and $magnitude(x)=\\sqrt{\\Sigma _i x_i^2}$.\n",
    "\n",
    "(See the [Wikipedia definition][1] for the formula, or [this page][2] for a detailed geometric explanation).\n",
    "\n",
    "The cosine similarity metric ranges between `-1.0` and `1.0`. Larger is better (smaller angle, i.e. more similar vectors). An ngram table compared to itself should return cosine `1.0`. Unrelated ngram tables should return cosine of around `0`.\n",
    "\n",
    "If an ngram does not appear in an ngram table, it has frequency 0 and contributes nothing to the numerator in the formula nor to the table's vector magnitude. So when comparing two tables,  trigrams that don't occur in either do not affect cosine measure because they do not contribute either to the dot product or the tables' vector magnitudes. This means that our calculations need only consider trigrams that occur in one of the tables: The dot product (numerator) is the sum over trigrams that are present on both sides (since other products are zero), and the magnitude of each vector (denominator) is calculated from its own trigrams.\n",
    "\n",
    "For example, the dot product of the following vectors is $2 * 2$, and each vector has magnitude $\\sqrt {2*2+1}$. The cosine score works out to $2*2/(\\sqrt {2*2+1}*\\sqrt {2*2+1})=4/5$.\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Cosine_similarity#Definition\n",
    "[2]: http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "REaFf61BCw-8"
   },
   "outputs": [],
   "source": [
    "from langdetect import cosine_similarity\n",
    "\n",
    "table1 = { \"<he\": 2, \"het\": 1 }\n",
    "table2 = { \"<he\": 2, \"hem\": 1 }\n",
    "score = cosine_similarity(table1, table2)\n",
    "\n",
    "# The score *should be* 0.8, but we get floating point error\n",
    "print(score)\n",
    "\n",
    "if abs(score - 4/5) < 0.000001:\n",
    "    print(\"ok\")\n",
    "else:\n",
    "    import sys\n",
    "    print(\"Incorrect result\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bCwQ2Fa0Cw_B"
   },
   "source": [
    "## 5. Script 1: Create profiles\n",
    "\n",
    "We're now ready to deal with collecting our trigram statistics. The script `write_profiles.py`  must define a function `make_profiles()` that accepts three arguments: The path to a folder containing the multilingual reference corpus, the path to an (empty) folder in which to write the trigram tables, and the (maximum) size of the trigram tables.\n",
    "\n",
    "When imported as a module, `write_profiles.py` should not read or write any files. When executed as a stand-alone script, it should run a single command\n",
    "\n",
    "    make_profiles(\"./training\", \"./trigram-models\", 200)\n",
    "\n",
    "### 5.1 The data\n",
    "\n",
    "Create the output folder `trigram-models` by hand, or make your program create it if it does not exist.<p/>\n",
    "\n",
    "The folder `training` contains a number of files with names like these:\n",
    "\n",
    "        Afrikaans-Latin1\n",
    "        Aguaruna-Latin1\n",
    "        Albanian-UTF8\n",
    "        (etc.)\n",
    "        \n",
    "The part before the hyphen is the language name. The part after the hyphen is the encoding. \n",
    "\n",
    "\n",
    "### 5.2 The function `make_profiles()`\n",
    "\n",
    "The function `make_profiles(datafolder, profilefolder, size)` must \n",
    "    \n",
    "  1. Loop over a list of the files in `datafolder`; for each file:\n",
    "  - Split each filename into language name and encoding.\n",
    "  - Read the file, using the appropriate encoding. \n",
    "  - Generate a table of the `size` (e.g.: 200) most frequent ngrams.\n",
    "  - Write out the table in a file in the `profilefolder`, named like this: `Afrikaans.200`.\n",
    "  \n",
    "### Notes\n",
    "\n",
    "* Use the functions of the `langdetect` module; do not reinvent any wheels!\n",
    "\n",
    "* All profiles must be saved in the `utf-8` encoding, regardless of the input's encoding.\n",
    "    \n",
    "* Check that your function works correctly if called with different directory names for input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PmLhmQx8IgFX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:File `'write_profiles.py'` not found.\n"
     ]
    }
   ],
   "source": [
    " %run write_profiles.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDkwNtKwCw_D"
   },
   "source": [
    "## 6. Script 2: The language recognizer\n",
    "\n",
    "To use our collection of trigram profiles for language identification, we will first create a dedicated class to manage them easily. Then we'll specify a command line interface for our recognizer. \n",
    "\n",
    "As usual, importing the script `matchlang.py` as a module should do no input, output, or long computations. When run as the main program, it must look for command line arguments and try to identify the language of the specified file.\n",
    "\n",
    "\n",
    "### 6.1 The `LangMatcher` class\n",
    "\n",
    "The `LangMatcher` class will allow a text be compared against multiple language profiles, in order to find the best match. It should have the following behavior:\n",
    "\n",
    "1. The initializer must have one required argument: The path to a directory with the saved trigram profiles. The initializer should define and store a dictionary whose keys are language names, and each value is itself a dictionary whose keys are ngrams and whose values are frequencies. These are read from the profile file with `langdetect.read_trigrams()`. (Use just the language name as the key, not the complete file name).\n",
    "\n",
    "3. `score(text, n=1, ngrams=200)`: Here, `text` is a string that has not yet been cleaned up or tokenized. Compute its ngram table, limited to `ngrams`  most frequent ngrams (default 200), and compare it against each of the languages in the model dictionary. Compile a list of the language names and cosine similarity scores, and return the names and similarity scores for the `n` best matches.\n",
    "\n",
    "5. `recognize(filename, encoding=\"utf-8\", ngrams=200)`: A convenience function: It opens the specified file, calls `score()` on its contents, and returns the top result (i.e., the name of the highest matching language and the similarity score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YZ8rp2BFCw_F"
   },
   "source": [
    "### 6.2 The recognizer main script\n",
    " \n",
    "When run as a main program, `matchlang.py` should do the following things:\n",
    "\n",
    "1. Accept one **or more** command line arguments; each is the path of a file whose language we want to identify. If there are **no** command line arguments, the program can do whatever you want. (It is a good idea to specify some default filenames to help with development and testing. Alternately you can do nothing, or print an error message.)\n",
    "\n",
    "2. If the option `-e` is given, its argument specifies the encoding to use for all test files. (E.g., `-e latin1`).  The default encoding is `utf-8`.\n",
    "\n",
    "**Program execution:**\n",
    "\n",
    "1. Initialize a `LangMatcher` object, filling it with all the profiles from the directory `trigram-models`.\n",
    "\n",
    "2. For each file specified on the command line, print out a line containing the file name, the most similar language, and its similarity score. \n",
    "\n",
    "The following code cell uses notebook \"magic\" commands (i.e., not Python) to run `matchlang.py` as a command line script in the directory that contains this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t3b_BxHfCw_G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "europarl-90/ep-00-02-03.nl\tDutch\t0.7273736039378357\n",
      "europarl-30/ep-00-01-17.da\tDanish\t0.3726819571803258\n",
      "europarl-30/ep-00-01-18.da\tDanish\t0.46092073189990596\n",
      "europarl-30/ep-00-01-21.da\tDanish\t0.4373769273603376\n"
     ]
    }
   ],
   "source": [
    "%run matchlang.py \"europarl-90/ep-00-02-03.nl\"\n",
    "\n",
    "%run matchlang.py europarl-30/*.da\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c6rH0UMtCw_N"
   },
   "source": [
    "## 7. Script 3: Evaluation\n",
    "\n",
    "The script `evaluate.py` assesses the performance of the language recognition.\n",
    "Since the tests are fixed, the directory names can be hard-coded in this file. \n",
    "\n",
    "We'll test our system on fragments from translations of European Parliament proceedings (`europarl` corpus). The algorithm works so well on clean, monolingual text, that mistakes are only likely with very short text fragments. Here we evaluate collections of randomly selected fragments that are 90, 30, and 10 words long. They are in the folders `europarl-90`, `europarl-30`, and `europarl-10`, respectively. All files are in `utf-8` format. The language of the fragment is encoded in the file names, but not in the same way as the training data: Each filename has a suffix indicating the ISO language code, e.g. `ep-00-02-02.de` (suffix `de`, German). The following codes are used:\n",
    "\n",
    "        da Danish\n",
    "        de German\n",
    "        el Greek\n",
    "        en English\n",
    "        es Spanish\n",
    "        fi Finnish\n",
    "        fr French\n",
    "        it Italian\n",
    "        nl Dutch\n",
    "        pt Portuguese\n",
    "        sv Swedish\n",
    "\n",
    "Convert the above table into a dictionary, and use it to automatically check if the language returned by the recognizer was correct.\n",
    "\n",
    "Write a function `eval(path)` that is given the pathname of a collection and performs language identification on each file of the collection. Print the name of each file and the recognizer's guess, and add the word \"ERROR\" and the correct language after incorrect guesses. Keep a tally of the number of correct and incorrect results, and report the totals at the end of each collection (separately for each collection).\n",
    "\n",
    "In the  main script, use your function `eval()` to evaluate the collections `europarl-90`, `europarl-30`, and `europarl-10`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Pq9q6GMCw_O"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test-clean/ep-00-01-17.da'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Documents\\GitHub\\ComputatieTrekkers\\evaluate.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m': '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' ERROR: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'europarl-90'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\GitHub\\ComputatieTrekkers\\evaluate.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mdir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test-clean/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mmatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatchlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindMath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\ComputatieTrekkers\\matchlang.py\u001b[0m in \u001b[0;36mfindMath\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfindMath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test-clean/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test-clean/ep-00-01-17.da'"
     ]
    }
   ],
   "source": [
    "%run evaluate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2DiPkM3eCw_R"
   },
   "source": [
    "## 8. Submission\n",
    "\n",
    "When you are done, upload your four Python files. If you wish to add any comments about problems or extensions to the tasks, add a file `README.txt` with the additional information.\n",
    "\n",
    "### Submission Checklist\n",
    "Before submission, please make sure you did not forget to include important parts of your code:\n",
    "- `langdetect.py` : core functions, and any other helper functions you find useful, including:\n",
    "    - `prepare()`\n",
    "    - `trigrams()`\n",
    "    - `trigram_table()`\n",
    "    - `read_table()` \n",
    "    - `write_table()`\n",
    "    - `cosine_similarity()`\n",
    "    \n",
    "- `write_profiles.py`: including function\n",
    "    -  `make_profiles()`\n",
    "    \n",
    "- `matchlang.py`: The language recognizer. This is a general-purpose program with command-line arguments and options.\n",
    "    - `MatchLang` class with `score()` and `recognize()` functions\n",
    "    - main script\n",
    "\n",
    "- `evaluate.py` evaluates your script on europarl-90, europarl-30, and europarl-10.\n",
    "    - `eval()`\n",
    "    -  main script"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment-langid.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
